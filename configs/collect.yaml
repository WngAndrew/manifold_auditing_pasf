#default config
# model_name: meta-llama/Meta-Llama-3.2-3B
# layer: 20
# batch_size: 32
# max_length: 256
# torch_dtype: float16
# prompts: data/prompts/prompts_10k.json

